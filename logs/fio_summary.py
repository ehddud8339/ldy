#!/usr/bin/env python3
import os
import json
import glob
import argparse
import pandas as pd


DEFAULT_INPUT = "./"
DEFAULT_OUTPUT = "fio_summary.xlsx"

# ================================
# ì›í•˜ëŠ” ì»¬ëŸ¼ì„ ì—¬ê¸°ì„œ ì„ íƒ (íŒŒì¼ëª… ì œì™¸)
# ì¡´ì¬í•˜ì§€ ì•Šì•„ë„ ë¬´ì‹œë¨
# ================================
SELECTED_COLUMNS = [
    "read_bw_MBps",
    "write_bw_MBps",
    "read_iops",
    "write_iops",
    "avg_latency_us",
    "p99_latency_us",
    "max_latency_us",
]
# ================================


def parse_filename(path: str):
    """
    filename ì˜ˆì‹œ:
       rr_cpu-pinned_fio.log
       thr_rand_iops_fio.log
    â†’ sched=rr, bound=cpu-pinned_fio (ë˜ëŠ” ë‚˜ë¨¸ì§€ ì „ë¶€)
    """
    base = os.path.basename(path)
    name, _ = os.path.splitext(base)

    parts = name.split("_", 1)
    sched = parts[0]
    bound = parts[1] if len(parts) > 1 else "default"

    return sched, bound


def load_fio_json(path: str):
    """
    fio ì‹¤í–‰ ì‹œ stderrì— ì—ëŸ¬/ê²½ê³ ê°€ ì„ì—¬ì„œ
    í•œ íŒŒì¼ì— "fio: ..." + JSON ì´ ê°™ì´ ë“¤ì–´ê°ˆ ìˆ˜ ìˆìŒ.

    ì´ í•¨ìˆ˜ëŠ”:
      1) íŒŒì¼ ì „ì²´ ë¬¸ìì—´ì„ ì½ê³ 
      2) ì²« ë²ˆì§¸ '{' ìœ„ì¹˜ì™€ ë§ˆì§€ë§‰ '}' ìœ„ì¹˜ë¥¼ ì°¾ì•„
      3) ê·¸ êµ¬ê°„ë§Œ ì˜ë¼ì„œ json.loads() ì‹œë„
    """

    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        text = f.read()

    start = text.find("{")
    end = text.rfind("}")

    if start == -1 or end == -1 or end <= start:
        print(f"[WARN] Cannot find valid JSON in: {path}")
        return None

    json_str = text[start : end + 1]

    try:
        return json.loads(json_str)
    except json.JSONDecodeError as e:
        print(f"[WARN] JSON decode failed for {path}: {e}")
        return None


def parse_fio_json(path: str):
    """
    FIO JSON íŒŒì‹±í•˜ì—¬ metrics dict ë°˜í™˜
    (jobs[] í•©ì‚°, latencyëŠ” weighted average)
    """
    data = load_fio_json(path)
    if not data:
        return {}

    jobs = data.get("jobs", [])
    if not jobs:
        return {}

    total_read_bw = 0.0
    total_write_bw = 0.0
    total_read_iops = 0.0
    total_write_iops = 0.0

    total_latency_ns = 0.0
    total_operations = 0
    max_latency_ns = 0.0
    p99_list = []

    for job in jobs:
        read = job.get("read", {}) or {}
        write = job.get("write", {}) or {}

        # BW bytes/sec â†’ MB/s
        total_read_bw += read.get("bw_bytes", 0.0) / (1024 * 1024)
        total_write_bw += write.get("bw_bytes", 0.0) / (1024 * 1024)

        # IOPS
        total_read_iops += read.get("iops", 0.0)
        total_write_iops += write.get("iops", 0.0)

        # latency (lat_ns)
        lat_ns = job.get("lat_ns", {}) or {}
        mean_lat = lat_ns.get("mean", 0.0)
        ops = read.get("total_ios", 0) + write.get("total_ios", 0)

        total_latency_ns += mean_lat * ops
        total_operations += ops

        # max latency
        max_latency_ns = max(max_latency_ns, lat_ns.get("max", 0.0))

        # clat percentilesì—ì„œ 99í¼ì„¼íƒ€ì¼
        # read ìª½ì— ìˆìœ¼ë©´ read ìš°ì„ , ì—†ìœ¼ë©´ writeì—ì„œ ê°€ì ¸ì˜´
        clat_read = job.get("read", {}).get("clat_ns", {}) or {}
        clat_write = job.get("write", {}).get("clat_ns", {}) or {}
        percentiles = clat_read.get("percentile") or clat_write.get("percentile") or {}

        if "99.000000" in percentiles:
            p99_list.append(percentiles["99.000000"])

    result = {}

    result["read_bw_MBps"] = round(total_read_bw, 3)
    result["write_bw_MBps"] = round(total_write_bw, 3)
    result["read_iops"] = round(total_read_iops, 2)
    result["write_iops"] = round(total_write_iops, 2)

    if total_operations > 0:
        avg_lat_ns = total_latency_ns / total_operations
        result["avg_latency_us"] = round(avg_lat_ns / 1000.0, 2)
    else:
        result["avg_latency_us"] = None

    if p99_list:
        p99_avg_ns = sum(p99_list) / len(p99_list)
        result["p99_latency_us"] = round(p99_avg_ns / 1000.0, 2)
    else:
        result["p99_latency_us"] = None

    result["max_latency_us"] = round(max_latency_ns / 1000.0, 2)

    return result


def main():
    parser = argparse.ArgumentParser(description="Parse FIO JSON logs (with noise) to XLSX")
    parser.add_argument(
        "-i", "--input",
        default=DEFAULT_INPUT,
        help="Input directory containing fio *.log/*.json (default: ./)",
    )
    parser.add_argument(
        "-o", "--output",
        default=DEFAULT_OUTPUT,
        help="Output XLSX filename (default: fio_summary.xlsx)",
    )

    args = parser.parse_args()

    log_files = glob.glob(os.path.join(args.input, "*.log")) + \
                glob.glob(os.path.join(args.input, "*.json"))

    rows = []

    for path in sorted(log_files):
        sched, bound = parse_filename(path)
        metrics = parse_fio_json(path)

        row = {
            "sched": sched,
            "bound": bound,
            "filename": os.path.basename(path),
        }
        row.update(metrics)
        rows.append(row)

    if not rows:
        print(f"âš  No FIO logs found in: {args.input}")
        return

    df = pd.DataFrame(rows)

    # ë©”íƒ€ ì»¬ëŸ¼
    front = ["sched", "bound", "filename"]

    # ì„ íƒëœ ì»¬ëŸ¼ ì¤‘ ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ
    metric_cols = [c for c in SELECTED_COLUMNS if c in df.columns]
    if not metric_cols:
        metric_cols = [c for c in df.columns if c not in front]

    df = df[front + metric_cols]

    # ì €ì¥
    os.makedirs(os.path.dirname(os.path.abspath(args.output)) or ".", exist_ok=True)

    with pd.ExcelWriter(args.output) as writer:
        for bound in sorted(df["bound"].unique()):
            subdf = df[df["bound"] == bound]
            sheet = bound[:31]
            subdf.to_excel(writer, sheet_name=sheet, index=False)

    print(f"ğŸ“ Saved â†’ {args.output}")
    print(f"ğŸ“Œ Columns: {', '.join(front + metric_cols)}")


if __name__ == "__main__":
    main()

